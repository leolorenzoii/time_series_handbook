{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhDinDS-M5 Leaderboards\n",
    "\n",
    "The following table presents the combined leaderboards of the (1) entire M5 competition, (2) entries of the the PhDinDS AIM students, and some (3) M5 benchamarks for comparison.\n",
    "\n",
    "For new entries, you may:\n",
    "1. Add a `.csv` file with the same structure and information in the `time_series_handbook/08_WinningestMethods/M5_competition/leaderboards_data` directory and fill out the necessary information (WRMSSE) per Level in the hierarchy.\n",
    "2. Re-run the code cell below.\n",
    "\n",
    "It is expected that these reported WRMSSE (M5 accuracy metric) for each entry is well-supported by its own `directory` (e.g. `phdinds2024_entry`) that details the methodology used in obtaining the forecasts. Description on the entries are shown in the sections below.\n",
    "\n",
    "Detailed description of the winningest methods, including their code are in the following repository:\n",
    "https://github.com/Mcompetitions/M5-methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:54:48.151500Z",
     "start_time": "2022-01-12T14:54:47.857236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Level1</th>\n",
       "      <th>Level2</th>\n",
       "      <th>Level3</th>\n",
       "      <th>Level4</th>\n",
       "      <th>Level5</th>\n",
       "      <th>Level6</th>\n",
       "      <th>Level7</th>\n",
       "      <th>Level8</th>\n",
       "      <th>Level9</th>\n",
       "      <th>Level10</th>\n",
       "      <th>Level11</th>\n",
       "      <th>Level12</th>\n",
       "      <th>AveWRMSSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YJ_STU</td>\n",
       "      <td>0.199000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.277000</td>\n",
       "      <td>0.365000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.474000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.573000</td>\n",
       "      <td>0.966000</td>\n",
       "      <td>0.929000</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Matthias</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.294000</td>\n",
       "      <td>0.416000</td>\n",
       "      <td>0.246000</td>\n",
       "      <td>0.349000</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>0.481000</td>\n",
       "      <td>0.497000</td>\n",
       "      <td>0.594000</td>\n",
       "      <td>1.023000</td>\n",
       "      <td>0.964000</td>\n",
       "      <td>0.907000</td>\n",
       "      <td>0.528000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mf</td>\n",
       "      <td>0.236000</td>\n",
       "      <td>0.319000</td>\n",
       "      <td>0.421000</td>\n",
       "      <td>0.308000</td>\n",
       "      <td>0.397000</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>0.496000</td>\n",
       "      <td>0.505000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.917000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.536000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>monsaraida</td>\n",
       "      <td>0.254000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.418000</td>\n",
       "      <td>0.302000</td>\n",
       "      <td>0.377000</td>\n",
       "      <td>0.411000</td>\n",
       "      <td>0.483000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.579000</td>\n",
       "      <td>0.963000</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.886000</td>\n",
       "      <td>0.536000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alan Lahoud</td>\n",
       "      <td>0.213000</td>\n",
       "      <td>0.324000</td>\n",
       "      <td>0.414000</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>0.416000</td>\n",
       "      <td>0.494000</td>\n",
       "      <td>0.503000</td>\n",
       "      <td>0.595000</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.897000</td>\n",
       "      <td>0.536000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phdinds2024</td>\n",
       "      <td>0.252662</td>\n",
       "      <td>0.358375</td>\n",
       "      <td>0.481902</td>\n",
       "      <td>0.357163</td>\n",
       "      <td>0.454832</td>\n",
       "      <td>0.466121</td>\n",
       "      <td>0.569269</td>\n",
       "      <td>0.581131</td>\n",
       "      <td>0.707473</td>\n",
       "      <td>1.283457</td>\n",
       "      <td>1.198084</td>\n",
       "      <td>1.147574</td>\n",
       "      <td>0.654837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ES_bu</td>\n",
       "      <td>0.426000</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.478000</td>\n",
       "      <td>0.577000</td>\n",
       "      <td>0.577000</td>\n",
       "      <td>0.654000</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>1.012000</td>\n",
       "      <td>0.969000</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ARIMA_td</td>\n",
       "      <td>0.615000</td>\n",
       "      <td>0.673000</td>\n",
       "      <td>0.753000</td>\n",
       "      <td>0.656000</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.856000</td>\n",
       "      <td>1.027000</td>\n",
       "      <td>0.969000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.796000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sNaive</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.673000</td>\n",
       "      <td>0.718000</td>\n",
       "      <td>0.623000</td>\n",
       "      <td>0.708000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.829000</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>1.223000</td>\n",
       "      <td>1.205000</td>\n",
       "      <td>1.176000</td>\n",
       "      <td>0.847000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ARIMA_bu</td>\n",
       "      <td>0.829000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.844000</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>0.882000</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.893000</td>\n",
       "      <td>0.938000</td>\n",
       "      <td>1.048000</td>\n",
       "      <td>0.981000</td>\n",
       "      <td>0.917000</td>\n",
       "      <td>0.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MA</td>\n",
       "      <td>0.891000</td>\n",
       "      <td>0.918000</td>\n",
       "      <td>0.931000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.964000</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.944000</td>\n",
       "      <td>0.988000</td>\n",
       "      <td>1.070000</td>\n",
       "      <td>1.006000</td>\n",
       "      <td>0.938000</td>\n",
       "      <td>0.956000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Entry    Level1    Level2    Level3    Level4    Level5    Level6  \\\n",
       "0       YJ_STU  0.199000  0.310000  0.400000  0.277000  0.365000  0.390000   \n",
       "1     Matthias  0.186000  0.294000  0.416000  0.246000  0.349000  0.381000   \n",
       "2           mf  0.236000  0.319000  0.421000  0.308000  0.397000  0.405000   \n",
       "3  monsaraida   0.254000  0.340000  0.418000  0.302000  0.377000  0.411000   \n",
       "4  Alan Lahoud  0.213000  0.324000  0.414000  0.272000  0.361000  0.416000   \n",
       "0  phdinds2024  0.252662  0.358375  0.481902  0.357163  0.454832  0.466121   \n",
       "5        ES_bu  0.426000  0.514000  0.580000  0.478000  0.577000  0.577000   \n",
       "8     ARIMA_td  0.615000  0.673000  0.753000  0.656000  0.768000  0.725000   \n",
       "6       sNaive  0.560000  0.673000  0.718000  0.623000  0.708000  0.760000   \n",
       "9     ARIMA_bu  0.829000  0.850000  0.870000  0.844000  0.905000  0.882000   \n",
       "7           MA  0.891000  0.918000  0.931000  0.900000  0.960000  0.964000   \n",
       "\n",
       "     Level7    Level8    Level9   Level10   Level11   Level12  AveWRMSSE  \n",
       "0  0.474000  0.480000  0.573000  0.966000  0.929000  0.884000   0.520000  \n",
       "1  0.481000  0.497000  0.594000  1.023000  0.964000  0.907000   0.528000  \n",
       "2  0.496000  0.505000  0.600000  0.950000  0.917000  0.875000   0.536000  \n",
       "3  0.483000  0.490000  0.579000  0.963000  0.928000  0.886000   0.536000  \n",
       "4  0.494000  0.503000  0.595000  0.995000  0.950000  0.897000   0.536000  \n",
       "0  0.569269  0.581131  0.707473  1.283457  1.198084  1.147574   0.654837  \n",
       "5  0.654000  0.643000  0.728000  1.012000  0.969000  0.915000   0.671000  \n",
       "8  0.810000  0.785000  0.856000  1.027000  0.969000  0.910000   0.796000  \n",
       "6  0.829000  0.801000  0.888000  1.223000  1.205000  1.176000   0.847000  \n",
       "9  0.932000  0.893000  0.938000  1.048000  0.981000  0.917000   0.908000  \n",
       "7  0.986000  0.944000  0.988000  1.070000  1.006000  0.938000   0.956000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "files = glob('leaderboards_data/*.csv')\n",
    "leaderboards = pd.concat([pd.read_csv(file) for file in files], axis=0)\n",
    "display(leaderboards.sort_values(by='AveWRMSSE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Description of Phd in DS entries\n",
    "\n",
    "**phdinds2024** A LightGBM model was trained for each *Level 9 (store-department)* series totalling to 70 trained models for the base level forecasts. Both the input structure and ML parameters for was optimized for each series. Specifically, lookback window, and delay were both optimized in addition to choosing whether to use a lookback on endogenous (unit sales), lookback on exogenous (calendar, sell prices, promotions, holiday data), and lookahead on the same exogenous variables. Simultaneously, LighGBM parameters on a `goss` boosting method: `top_rate`, `other_rate`, `tree_learner`, `n_estimators`, `learning_rate`, and `num_leaves` were optimized for `rmse` on a `regression` objective. No early stopping was empoyed. *Reconciliation*: These Level 9 forecasts were then used to obtain Levels 1 to 8 using bottom-up (bu) reconciliation and to obtain Levels 10 to 12 using top-down (td, based on average proportions). For details see, `phdinds2024_entry/phdinds2024_entry.ipynb`.\n",
    "\n",
    "\n",
    "*Describe new entries here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Description of M5 Winningest Methods\n",
    "\n",
    "*Descriptions are lifted from Makridakis, et al (2020)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1st:** `0.520` *YJ_STU* Senior undergraduate student in SK - LightGBM models were trained to produce forecasts for the product-store series using data per store (10 models), store-category (30 models), and store-department (70 models). Two variations: recursive and non-recursive. Total of 220 models were built and each series was forecast using the average of 6 models. Each one exploiting a different learning approach and train set. The models were optimized without considering early stopping and\n",
    "by maximizing the neg log-likelihood of the Tweedie Distribution (Zhou et al., 2020). The method was fine-tuned using the last four 28-day-long\n",
    "    windows of available data for CV and by measuring both the mean and the standard deviation of the errors produced by the individual models and their combinations. Features used were calendar-related information, special events, promotions, prices,\n",
    "and unit sales data, both in a recursive and a non-recursive format.\n",
    "\n",
    "**2nd:** `0.528` *Matthias* LightGBM + N-BEATS (DL for time series forecasting). This method was also based on an equally weighted\n",
    "combination of various LightGBM models, however, was externally adjusted through multipliers according\n",
    "to the forecasts produced by N-BEATS (deep-learning NN for time series forecasting; (Oreshkin\n",
    "et al., 2019)) for the top fineve aggregation levels of the dataset. Essentially, LightGBM models were\n",
    "first trained per store (10 models) and then five different multipliers were used to adjust their forecasts\n",
    "and properly capture the trend. In this regard, a total of 50 models were built and each series of the\n",
    "product-store level of the dataset was forecast using a combination of fiveve different models. The loss\n",
    "function used was a custom, asymmetric one. The last four 28-day-long windows of available data\n",
    "were used for CV and model building. The LightGBM models were trained using only some basic\n",
    "features about calendar effects and prices (past unit sales were not considered), while the N-BEATS\n",
    "model was based solely on historical unit sales.\n",
    "\n",
    "**3rd:** `0.536` *mf* LSTM. This method involved an equally weighted\n",
    "combination of 43 deep-learning NNs (Salinas et al., 2020), each consisting of multiple LSTM layers\n",
    "that were used to recursively predict the product-store series. From the models trained, 24 considered\n",
    "dropout, while the remaining 19 did not. Note that these models originated from just 12 models\n",
    "and corresponded to the last, more accurate instances observed for these models while training, as\n",
    "specified through CV (last fourteen 28-day-long windows of available data). Similar to the winner,\n",
    "the method considered Tweedie regression, but was modifieded however to optimize weights based on\n",
    "sampled predictions instead of actual values. The Adam optimizer and the cosine annealing was used\n",
    "for the learning rate schedule. The NNs considered a total of 100 features of similar nature to those of\n",
    "the winning submission (sales data, calendar-related information, prices, promotions, special events,\n",
    "identifiers, and zero-sales periods).\n",
    "\n",
    "\n",
    "**4th:** `0.536` *monsaraida* Weekly forecasting of the horizon (28 days chopped into weeks) using LGBM trained on store-level data. This method produced forecasts for the\n",
    "product-store series of the dataset using non-recursive LightGBM models, trained per store (10 models).\n",
    "However, in contrast to the rest of the methods, each week of the forecasting horizon was forecast\n",
    "separately using a different model (4 models per store). Thus, a total of 40 models were built to\n",
    "produce the forecasts. The features used as inputs were similar to those of the winning submission,\n",
    "with the exception of the recursive ones. Tweedie regression was considered for training the models,\n",
    "with no early stopping, and no optimization was performed in terms of training parameters. The last\n",
    "five 28-day-long windows of available data were used for CV.\n",
    "\n",
    "\n",
    "**5th:** `0.536` *Alan Lahoud* This method considered recursive LightGBM models,\n",
    "trained per department (7 models). After producing the forecasts for the product-store series, these\n",
    "were externally adjusted so that that the mean of each of the series at the store-department level was\n",
    "the same as the one of the previous 28 days. This was done using appropriate multipliers. The models\n",
    "were trained using Poisson regression with early stopping and validated using a random sample of 500\n",
    "days. The features used as input were similar to those of the winning submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Description of M5 Benchmarks\n",
    "\n",
    "*Descriptions are lifted from Makridakis, et al (2020)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ES_bu** An algorithm is used to automatically\n",
    "select the most appropriate exponential smoothing model for predicting total sales (level 1),\n",
    "indicated through information criteria (Hyndman et al., 2002). Then, the rest of\n",
    "the series (levels 1-11) are predicted using the bottom-up method.\n",
    "\n",
    "**ARIMA_td** An algorithm is used to automatically select the most appropriate ARIMA model for predicting total\n",
    "sales (level 1), indicated through information criteria (Hyndman & Khandakar, 2008). Then, the rest\n",
    "of the series (levels 1-11) are predicted using the bottom-up method.\n",
    "\n",
    "**ARIMA_bu** The same algorithm used in ARIMA td is employed for forecasting the product-store series of the\n",
    "dataset (level 12). Then, the rest of the series (levels 1-11) are predicted using the bottom-up method.\n",
    "\n",
    "**sNaive** The forecasts at time t are equal to the last known observation of the\n",
    "same period, $t - m$, as follows:\n",
    "$$\n",
    "\\hat{y}_t = y_{t-m}\n",
    "$$\n",
    "\n",
    "where m is the frequency of the series. In M5, m is set equal to 7 since the series are daily. Contrary\n",
    "to the Naive method, sNaive can capture possible seasonal variations. Although sales do not usually\n",
    "display strong seasonality at low cross-sectional levels, this is very likely at higher aggregation levels.\n",
    "\n",
    "**MA** Moving averages are often used in practice to forecast sales (Syntetos &\n",
    "Boylan, 2005). Forecasts are computed by averaging the last k observations of the series as follows:\n",
    "\n",
    "$$\n",
    "\\hat{y}_t = \\frac{\\Sigma_{i=1}^{k} y_{t-i}}{k}\n",
    "$$\n",
    "\n",
    "The order of the MA ranges between 2 and 14 and is specifieded by minimizing the in-sample MSE of\n",
    "the method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Hyndman, R. J., Koehler, A. B., Snyder, R. D., & Grose, S. (2002). A state space framework for automatic forecasting using\n",
    "exponential smoothing methods. *International Journal of Forecasting*, 18, 439-454.\n",
    "\n",
    "Hyndman, R., & Khandakar, Y. (2008). Automatic time series forecasting: the forecast package for R. *Journal of Statistical\n",
    "Software*, 26, 1-22.\n",
    "\n",
    "Syntetos, A. A., Boylan, J. E., & Croston, J. D. (2005). On the categorization of demand patterns. *Journal of the Operational\n",
    "Research Society*, 56, 495-503.\n",
    "\n",
    "Makridakis, S., Spiliotis, E., and Assimakopoulos, V. (2020). The M5 Accuracy Competition: Results, findings, and conclusions. *International Journal of Forecasting*\n",
    "\n",
    "Makridakis, S., & Spiliotis, E. (2021). The M5 Competition and the Future of Human Expertise in Forecasting. *Foresight: The International Journal of Applied Forecasting*, (60)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
